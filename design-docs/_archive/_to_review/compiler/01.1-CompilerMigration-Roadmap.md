Below is a step-by-step, “leave nothing out” roadmap from your current compiler (wire/bus aware, closure-heavy Program) to the “do it right” architecture:

Compiled IR + scheduled runtime + value store
optimized for performance, determinism, debuggability, and a clean Rust/WASM path.

I’m going to treat this as a single path (no MVP forks, no “keep both forever”), but I will stage it so you can keep running the app the whole time.

⸻

Phase 0 — Lock invariants and define the target runtime contract

0.1 Define the one true runtime interface (the end-state)

You want a runtime that does not rely on JS closure graphs as the primary representation.

Define these as non-negotiable contracts (even if initially implemented in JS):
	•	IR module: immutable, serializable, stable IDs
	•	Schedule: dense arrays of ops with numeric IDs
	•	ValueStore: typed storage keyed by dense IDs
	•	Evaluator: executes schedule for a frame, writes results into ValueStore
	•	Program: { ir, schedule, valueStoreLayout, eval(frameCtx) }

0.2 Decide the canonical ID spaces (now)

This is foundational for performance + Rust.
	•	BlockId -> blockIx (u32)
	•	PortKey (blockId:portName) -> portIx (u32)
	•	BusId -> busIx (u32)
	•	Publisher/Listener -> bindingIx (u32)
	•	Op -> opIx (u32)

Lock:
	•	IDs are assigned at compile time.
	•	Runtime never uses strings except for debug/UI lookup.

0.3 Decide the canonical value representation classes

Pick a small set that maps cleanly to Rust/WASM:
	•	Scalar: f32/f64/i32/u32/bool (+ small structs like vec2/vec3)
	•	Signal: not a closure; it’s an expr node evaluated per frame (or per sample)
	•	Field: not “array now”; it’s a FieldExpr with explicit materialization ops
	•	Special: Domain, RenderTree, etc. but still stored behind typed handles

This is where you commit to:
	•	SignalExpr (recommended) + FieldExpr
	•	materialization is an op (never implicit inside random JS functions)

⸻

Phase 1 — Unify types and ports so the compiler can emit numeric IR cleanly

1.1 Make Editor ports and Compiler ports the same concept

Right now you have divergence (editor types, compiler types, plus mapping glue). Before IR, unify:
	•	Create one canonical TypeDesc / PortType used by both editor and compiler.
	•	Make BlockDefinition ports (inputs/outputs) derive from a single schema used everywhere.

Deliverables:
	•	A single PortRef representation and one canonical TypeDesc.
	•	A single “port key” scheme used by compilation, runtime, and debug index.

1.2 Kill stringly typed “artifact maps” inside compiler passes

Replace:
	•	Map<string, Artifact> keyed by "blockId:port"

With:
	•	Artifact[] keyed by portIx (dense index)

You can still maintain a DebugIndex map for UI.

This is the first major speed win and the first major “Rust path” enabler.

⸻

Phase 2 — Introduce IR as an internal compiler product while still using closures as leaf ops

This is the “bridge phase”: you start emitting the IR and schedule without immediately rewriting every block compiler.

2.1 Define the IR module schema (immutable)

Include:
	•	type tables
	•	node tables (SignalExpr, FieldExpr, Domain ops, Render ops)
	•	bus graph (publishers/listeners, combine nodes)
	•	debug metadata (source spans, block labels, etc)

2.2 Define the Schedule op set (explicit runtime operations)

Your schedule should be a compact list of ops like:
	•	EvalSignalExpr(outValueIx, exprRootIx, tIx, ctxIx)
	•	EvalFieldExpr(outValueIx, fieldRootIx, domainIx, seedIx, nIx)
	•	BusCombine(outBusValueIx, busIx, inputs[], combineMode)
	•	ApplyAdapter(outIx, inIx, adapterIx, paramsIx)
	•	ApplyLens(outIx, inIx, lensIx, paramsIx)
	•	RenderSink(outRenderIx, sinkIx, inputs...)
	•	EmitRenderTree(outRenderIx)

Even if the initial implementation calls back into existing closures, the shape is now explicit.

2.3 Create a new evaluator that runs a schedule (JS implementation first)
	•	It loops for op in schedule: run(op)
	•	It reads/writes only numeric-indexed values in the ValueStore.

At this stage, some ops may call existing closure-based implementations:
	•	EvalSignalExpr might call a wrapper that calls an old Program<number> closure.
	•	EvalFieldExpr might call your current Field(seed,n,ctx) function.

That’s OK. The key is: evaluation is centralized and schedulable.

2.4 Build the ValueStore (JS typed arrays where possible)

Create a ValueStore layout decided at compile time:
	•	scalarsF32: Float32Array
	•	scalarsF64: Float64Array (if needed)
	•	vec2: Float32Array (packed)
	•	handles: Uint32Array (for “pointer-like” refs: Domain, RenderTree nodes, Field handles)
	•	objects: any[] only where you truly must (RenderTree in JS, until you port)

A good rule:
	•	Anything performance-critical becomes typed arrays.
	•	“Big structured trees” can stay in JS objects initially, but you must isolate them behind handles.

⸻

Phase 3 — Recompile pipeline refactor: compiler emits IR + schedule, not closures

3.1 Replace “compile blocks to Artifacts” with “compile blocks to IR fragments”

Each block compiler changes from:

“Return Artifact = closure / function / value”

To:

“Return IR nodes + output ValueRefs (valueIx handles) + schedule snippets”

In concrete terms:
	•	A block compiler emits:
	•	SignalExpr/FieldExpr nodes (or references)
	•	adapter/lens steps as IR nodes
	•	optional constants as IR constants
	•	the mapping from the block’s output ports to value indices

3.2 Build an explicit dependency graph over value indices

You already build a DAG over blocks/wires/buses. Now do it over ops and values:
	•	Node = operation that produces a value
	•	Edge = reads from upstream valueIx

This is where determinism gets nailed down:
	•	stable traversal order
	•	stable combine ordering
	•	stable tie-breakers

3.3 Schedule generation becomes the primary output of compilation

Compilation output becomes:
	•	IRModule
	•	Schedule[]
	•	ValueStoreLayout
	•	DebugIndex

No “Program closure” anywhere.

⸻

Phase 4 — Make buses first-class IR (no more “hidden combine inside closures”)

4.1 Compile bus pipelines into explicit ops

For each bus:
	•	gather publishers (sorted by sortKey)
	•	compile publisher adapter/lens stacks into ops
	•	emit a BusCombine op
	•	store result in a bus value slot (valueIx)

4.2 Listeners become explicit reads

A listener connection becomes:
	•	valueIx(inputPort) = maybe(adapters/lenses)(valueIx(busResult))

Now the debugger can show:
	•	publisher A contributed X
	•	adapter chain produced Y
	•	combine produced Z
	•	listener consumed Z

…and it can do so without guesswork.

⸻

Phase 5 — Make Signal “real” (SignalExpr), not closures

This is the critical Rust/WASM enabler.

5.1 Replace runtime Program<T> closures with SignalExpr AST

A Signal is:
	•	constants
	•	time reads (TimeRoot outputs)
	•	math ops
	•	state reads (from state blocks)
	•	bus reads
	•	adapters/lenses

This requires:
	•	a canonical op library (“kernel ops”) for signals
	•	a stable numeric encoding for each op kind

5.2 State blocks become explicit stateful ops (no hidden closure state)

Instead of a closure capturing state, define:
	•	StateSlotIx allocated in ValueStoreLayout
	•	ops like IntegrateStep(stateIx, inValueIx, dtIx)
	•	schedule ensures ordering
	•	determinism ensured by schedule

Now “port to Rust” is straightforward because “state” is explicit storage.

⸻

Phase 6 — Make Field “real” (FieldExpr + explicit materialization)

You already want lazy fields; this is where it becomes systematic.

6.1 FieldExpr is a structural graph, not a function returning arrays

FieldExpr nodes represent:
	•	const
	•	elementIndex / elementId reads
	•	domain fields (pos, row, col, etc.)
	•	map/zip/reduce-like patterns (careful: avoid reduce unless explicit)
	•	bus reads (field buses)
	•	adapters/lenses (field-domain versions)

6.2 Materialization is only allowed at specific ops (render sinks, export)

Define:
	•	MaterializeField(fieldExprIx, domainIx, outBufferHandleIx, captureStats?)

Renderer consumes buffers, not closures.

This is how you prevent accidental quadratic work and how you get reliable perf tracing.

⸻

Phase 7 — Renderer and RenderTree: isolate structured output behind handles

7.1 Introduce a Render IR / Render command buffer

Even if you keep producing a JS RenderTree initially, you want the evaluator to produce a render command stream:
	•	instance buffers + draw calls + effect params

In JS, you can translate that into your existing RenderTree for now.

In Rust, the same buffer becomes GPU-friendly.

7.2 Render sinks become pure consumers of ValueStore

A render sink op should:
	•	read Domain handle
	•	ensure required Fields are materialized (via ops earlier in schedule)
	•	build render commands deterministically
	•	emit one handle to the final “frame output”

⸻

Phase 8 — Debugger instrumentation becomes trivial (and cheap)

Once evaluation is scheduled + centralized, you don’t need invasive wrapping.

8.1 Add span hooks at op boundaries
	•	begin/end span per op
	•	optional capture per op kind
	•	ring buffer write only (no allocations)

8.2 Emit causal edges at the moment an op runs
	•	op knows its input value indices and output value index
	•	write Edge{out,in,relation,ordinal} to ring

This is the only correct way to guarantee the “why graph”.

⸻

Phase 9 — Determinism hardening and hot-swap rules

9.1 Determinism checklist enforced by compiler
	•	schedule order stable
	•	bus publisher order stable (sortKey)
	•	tie-breaking stable (by numeric IDs)
	•	floating point policy documented (f32 vs f64)

9.2 Program swap compatibility rules

When you recompile:
	•	build new IR/schedule/layout
	•	attempt to “rebind” state slots where semantics match
	•	keep time continuous
	•	define exact rules for what resets state vs what migrates

This is where your “no-jank live edits” becomes enforceable, not aspirational.

⸻

Phase 10 — Rust/WASM path becomes a packaging problem, not a rewrite

At this point:
	•	IR is serializable
	•	schedule is numeric and dense
	•	ValueStore is typed
	•	evaluator is centralized

So the Rust port is:
	1.	implement the evaluator in Rust
	2.	implement the same op set
	3.	run schedule over ValueStore
	4.	send render commands back to JS (or directly to GPU depending on your environment)

No redesign required.

⸻

Practical sequencing (what to do first, week-by-week style)

If you want the “least regret” order:
	1.	Numeric IDs + DebugIndex + portIx artifacts (big win, low risk)
	2.	Schedule + centralized evaluator (even if leaf ops call old closures)
	3.	Bus ops explicit (combine/adapter/lens as schedule nodes)
	4.	ValueStore typed arrays + eliminate Map lookups in hot path
	5.	SignalExpr (rip out closure Signals)
	6.	State blocks become explicit state slots (determinism + Rust)
	7.	FieldExpr + explicit materialization (perf + debugger + Rust)
	8.	Render command buffer (perf + portability)

⸻

If you want, tell me which of these is already partially in-progress in your repo (even informally), and I’ll turn this into an implementation checklist with concrete file-level change boundaries and “definition of done” for each step.